import torch
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
import base64
import ollama


# Detectar si hay GPU disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Usando dispositivo: {device}")

# -----------------------
# PARTE 1: Clasificaci√≥n
# -----------------------

# Transformaciones para la clasificaci√≥n
clasificacion_transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Cargar el modelo entrenado de clasificaci√≥n
modelo_path = "F1MarshAIl.pth"  # Aseg√∫rate de que este archivo existe
modelo = models.resnet152()
num_ftrs = modelo.fc.in_features
modelo.fc = nn.Linear(num_ftrs, 2)
modelo.load_state_dict(torch.load(modelo_path, map_location=device))
modelo.to(device)
modelo.eval()

# Definir las clases y mensajes personalizados
clases = ["con_sancion", "sin_sancion"]
mensajes = {
    "con_sancion": "üî¥ La siguiente acci√≥n es sancionable.",
    "sin_sancion": "‚úÖ La siguiente acci√≥n no es sancionable, es un lance de carrera."
}

def clasificar_imagen(image_path):
    """
    Clasifica la imagen y devuelve un mensaje basado en la categor√≠a.
    """
    image = Image.open(image_path).convert('RGB')
    image = clasificacion_transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        output = modelo(image)
        _, pred = torch.max(output, 1)
    categoria = clases[pred.item()]
    return mensajes[categoria], categoria

# -----------------------
# PARTE 2: Integraci√≥n de Clasificaci√≥n y Descripci√≥n con Bakllava
# -----------------------

def analizar_imagen(image_path):
    """
    Clasifica la imagen y, adem√°s, utiliza Bakllava para obtener una descripci√≥n detallada.
    """
    # mensaje, categoria = clasificar_imagen(image_path)
    # print("\nüîç An√°lisis de la imagen:")
    # print(f"‚û°Ô∏è Clasificaci√≥n: {mensaje}")
    
    # Obtener descripci√≥n detallada usando Bakllava
    # Se asume que 'describe_image' es el m√©todo que genera la descripci√≥n de la imagen
    # descripcion_detallada = bakllava_embeddings.describe_image(image_path)
    # print("‚û°Ô∏è Descripci√≥n detallada:", descripcion_detallada)

    """ Generar una descripci√≥n detallada del accidente usando BakLLaVA"""
    with open(image_path, "rb") as img_file:
        encoded_image = base64.b64encode(img_file.read()).decode("utf-8")
    
    prompt = """
        Analiza la imagen y describe objetivamente la acci√≥n ocurrida. **No determines si es sancionable o no.**  
        Responde en este formato exacto:  

        1. **N√∫mero de coches involucrados**: [n√∫mero]  
        2. **Colores de los coches**: [color1] y [color2]  
        3. **Zona de la colisi√≥n**: [parte del coche golpeada, ej. "lateral derecho", "frontal contra trasero"]  
        4. **Posici√≥n relativa**: [¬øEstaban en paralelo? ¬øUno por delante del otro?]  
        5. **Contexto de la acci√≥n**: [¬øQu√© estaba pasando antes del contacto? Ej: "Uno intentaba adelantar", "Frenada brusca"]  
        6. **Posible culpable**: [¬øQuien podr√≠a ser el posible culpable?]

        Descripci√≥n adicional: [Breve resumen]  
        """

    response = ollama.generate(
        model="llava",  # Alternativa m√°s estable
        prompt=prompt,
        images=[encoded_image]
    )

    return response.get("response", "Error al generar la descripci√≥n")

# Prueba con una imagen
ruta_imagen = "./pruebas/frame_83.jpg"  # Cambia por la ruta de tu imagen
resultado = analizar_imagen(ruta_imagen)
print(resultado)
